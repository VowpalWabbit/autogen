{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_groupchat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Generated Agent Chat: Group Chat\n",
    "\n",
    "AutoGen offers conversable agents powered by LLM, tool or human, which can be used to perform tasks collectively via automated chat. This framework allows tool use and human participation through multi-agent conversation.\n",
    "Please find documentation about this feature [here](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat).\n",
    "\n",
    "This notebook is modified based on https://github.com/microsoft/FLAML/blob/4ea686af5c3e8ff24d9076a7a626c8b28ab5b1d7/notebook/autogen_multiagent_roleplay_chat.ipynb\n",
    "\n",
    "## Requirements\n",
    "\n",
    "AutoGen requires `Python>=3.8`. To run this notebook example, please install:\n",
    "```bash\n",
    "pip install pyautogen\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "# %pip install pyautogen~=0.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install learn-to-pick"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To capture the new agent during dev\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import autogen\n",
    "\n",
    "config_list_gpt4 = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    file_location=\"..\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4\", \"gpt-4-0314\", \"gpt4\", \"gpt-4-32k\", \"gpt-4-32k-0314\", \"gpt-4-32k-v0314\"],\n",
    "    },\n",
    ")\n",
    "# config_list_gpt35 = autogen.config_list_from_json(\n",
    "#     \"OAI_CONFIG_LIST\",\n",
    "#     filter_dict={\n",
    "#         \"model\": {\n",
    "#             \"gpt-3.5-turbo\",\n",
    "#             \"gpt-3.5-turbo-16k\",\n",
    "#             \"gpt-3.5-turbo-0301\",\n",
    "#             \"chatgpt-35-turbo-0301\",\n",
    "#             \"gpt-35-turbo-v0301\",\n",
    "#         },\n",
    "#     },\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It first looks for environment variable \"OAI_CONFIG_LIST\" which needs to be a valid json string. If that variable is not found, it then looks for a json file named \"OAI_CONFIG_LIST\". It filters the configs by models (you can filter by other keys as well). Only the gpt-4 models are kept in the list based on the filter condition.\n",
    "\n",
    "The config list looks like the following:\n",
    "```python\n",
    "config_list = [\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your OpenAI API key here>',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'api_base': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-4-32k',\n",
    "        'api_key': '<your Azure OpenAI API key here>',\n",
    "        'api_base': '<your Azure OpenAI API base here>',\n",
    "        'api_type': 'azure',\n",
    "        'api_version': '2023-06-01-preview',\n",
    "    },\n",
    "]\n",
    "```\n",
    "\n",
    "If you open this notebook in colab, you can upload your files by clicking the file icon on the left panel and then choose \"upload file\" icon.\n",
    "\n",
    "You can set the value of config_list in other ways you prefer, e.g., loading from a YAML file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = {\"config_list\": config_list_gpt4, \"seed\": 42}\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "   name=\"User_proxy\",\n",
    "   system_message=\"A human admin.\",\n",
    "   code_execution_config={\"last_n_messages\": 2, \"work_dir\": \"groupchat\"},\n",
    "   human_input_mode=\"TERMINATE\"\n",
    ")\n",
    "coder = autogen.AssistantAgent(\n",
    "    name=\"Coder\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "pm = autogen.AssistantAgent(\n",
    "    name=\"Product_manager\",\n",
    "    system_message=\"Creative in software product ideas.\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "groupchat = autogen.GroupChat(agents=[user_proxy, coder, pm], messages=[], max_round=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import oai\n",
    "\n",
    "class autogen_llm_call:\n",
    "    def __init__(self, config_list):\n",
    "        self.config_list = config_list\n",
    "    def predict(self, message):\n",
    "        response = oai.ChatCompletion.create(\n",
    "            config_list=self.config_list,\n",
    "            prompt=message,\n",
    "        )\n",
    "        return oai.ChatCompletion.extract_text(response)[0]\n",
    "\n",
    "autogen_llm_call(config_list=config_list_gpt4).predict(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from autogen.agentchat.contrib.learn_to_pick_agent import LearnToPickAgent\n",
    "import learn_to_pick\n",
    "\n",
    "# scoring_criteria_template = \"\"\"Given message history: \"{message_history}\", the previously selected agent: \"{last_speaker}\" and the agents that could answer this message: \"{agent}\" rank how good or bad this agent selection is: \"{picked}\" \"\"\"\n",
    "\n",
    "ltp_config = {\n",
    "    # \"selection_scorer\": learn_to_pick.AutoSelectionScorer(llm=autogen_llm_call(config_list=config_list_gpt4), scoring_criteria_template_str=scoring_criteria_template)\n",
    "    \"selection_scorer\": None,\n",
    "}\n",
    "\n",
    "ltp_agent = LearnToPickAgent(\n",
    "    llm_config=llm_config,\n",
    "    learn_to_pick_config=ltp_config,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hacky way to set the agent\n",
    "groupchat.ltp_agent = ltp_agent\n",
    "groupchat.previous_select_speaker_func = groupchat.select_speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_select_speaker(self, last_speaker: autogen.Agent, selector: autogen.ConversableAgent):\n",
    "    \n",
    "    # NOTE: Use with custom selection scorer set to AutoSelectionScorer (in config above)\n",
    "\n",
    "    # The goal here is to demonstrate the usage of ltp agent in the role of the\n",
    "    # deciding which agent needs to go next\n",
    "\n",
    "    agent_desc = [{\"name\": agent.name, \"desc\": agent.system_message} for agent in self.agents]\n",
    "    \n",
    "    # TODO: Embed here should be exposed through LearnToPickAgent and not need to use\n",
    "    # learn_to_pick directly\n",
    "    msg = {\n",
    "        \"content\": {\n",
    "            \"agent\": learn_to_pick.Embed(LearnToPickAgent.ToSelectFrom(agent_desc)),\n",
    "            \"message\": learn_to_pick.Embed(LearnToPickAgent.BasedOn(self.messages[-1][\"content\"])),\n",
    "            \"last_speaker\": LearnToPickAgent.BasedOn(last_speaker.name),\n",
    "            \"message_history\": self.messages,\n",
    "        }\n",
    "    }\n",
    "    self.ltp_agent.receive(message=msg, sender=ltp_agent, request_reply=True, silent=True)\n",
    "    reply = list(ltp_agent.chat_messages.values())[-1][-1][\"content\"]\n",
    "    if \"picked\" in reply:\n",
    "        agent_name = reply[\"picked\"][\"agent\"][\"name\"]\n",
    "        print(f\"agent name picked: --{agent_name}--\")\n",
    "        try:\n",
    "            return self.agent_by_name(str(agent_name))\n",
    "        except ValueError as e:\n",
    "            print(f\"agent does not exist? got ValueError: {agent_name}, error: {e}\")\n",
    "            return self.next_agent(last_speaker, agents)\n",
    "    else:\n",
    "        print(f\"no agent picked, problem with calling ltp agent?\")\n",
    "        return self.next_agent(last_speaker, agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_select_speaker_warm_start(self, last_speaker: autogen.Agent, selector: autogen.ConversableAgent):\n",
    "    \n",
    "    # NOTE: Use with selection_scorer set to None in the config\n",
    "    # The goal here is to demonstrate that the ltp agent is learning \"warm start\" from what\n",
    "    # AutoGen GroupChat manager is doing\n",
    "    # Can save progress and resume (not demonstrated below, not sure I exposed that in the agent yet)\n",
    "\n",
    "    # What did the group manager pick?\n",
    "    group_chat_agent_selection = self.previous_select_speaker_func(last_speaker, selector)\n",
    "    \n",
    "    agent_desc = [{\"name\": agent.name, \"desc\": agent.system_message} for agent in self.agents]\n",
    "    msg = {\n",
    "        \"content\": {\n",
    "            \"agent\": learn_to_pick.Embed(LearnToPickAgent.ToSelectFrom(agent_desc)),\n",
    "            \"message\": learn_to_pick.Embed(LearnToPickAgent.BasedOn(self.messages[-1][\"content\"])),\n",
    "            \"last_speaker\": LearnToPickAgent.BasedOn(last_speaker.name),\n",
    "            \"message_history\": self.messages,\n",
    "        }\n",
    "    }\n",
    "    self.ltp_agent.receive(message=msg, sender=ltp_agent, request_reply=True, silent=True)\n",
    "    ltp_reply = list(ltp_agent.chat_messages.values())[-1][-1][\"content\"]\n",
    "    if \"picked\" in ltp_reply:\n",
    "        ltp_agent_name = ltp_reply[\"picked\"][\"agent\"][\"name\"]\n",
    "        print(f\"agent name picked by ltp agent: --{ltp_agent_name}--\")\n",
    "        \n",
    "        if group_chat_agent_selection.name == ltp_agent_name:\n",
    "            ltp_agent.learn_from_user_feedback(1.0, ltp_reply)\n",
    "        else:\n",
    "            # -1 or 0.0 here?\n",
    "            ltp_agent.learn_from_user_feedback(0.0, ltp_reply)\n",
    "    \n",
    "    return group_chat_agent_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupchat.select_speaker = custom_select_speaker.__get__(groupchat)\n",
    "groupchat.select_speaker = custom_select_speaker_warm_start.__get__(groupchat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_proxy.initiate_chat(manager, message=\"Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\")\n",
    "# type exit to terminate the chat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flaml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
